# Predicting How Well Weight Lifting Exercises Are Performed 

Brian Buckley
Sunday, January 25 2015

***

## Objective

The purpose of this exercise is to use the weight lifting data set of the [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har) 
program from [Groupware](http//groupware.les.inf.puc-rio.br).  In these data, 6 participants performed a weight lifting exercise in 5 different 
ways whereby one of the five was the correct technique.  Data from a set of accelerometers was collected and a machine learning algorithm used 
to predict if the user was performing the weight lifting correctly.

Our objective is to use the "classe" variable in the training set to predict the manner in which Groupware did the exercise.

***

## Data

We loaded the data from the two links provided.  

```{r echo=FALSE}
setwd("C:/Users/buckleyb/Documents/Personal/Courses/Johns Hopkins Practical Machine Learning/Code Assignment")
if (!file.exists("data")) {dir.create("data")}
```
```{r tidy=TRUE}
library(RCurl)
data1 <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",ssl.verifypeer=0L, followlocation=1L)
writeLines(data1,'./data/pml-training.csv')
data2 <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",ssl.verifypeer=0L, followlocation=1L)
writeLines(data2,'./data/pml-testing.csv')
dateDownloaded <- date()
```
When we look at the data using str() we see that it has 19622 observations on 160 variables.
We also note a lot of NA and empty columns plus some irrelevant variables like user name, timestamp, etc.
So first we cleaned it up by removing all NAs

```{r tidy=TRUE}
training1 <- read.csv("./data/pml-training.csv", na.strings= c("NA",""," "))
training2 <- apply(training1, 2, function(x) {sum(is.na(x))})
training3 <- training1[,which(training2 == 0)]
```

Next we removed identifier columns such as name, timestamps etc

```{r tidy=TRUE}
training3 <- training3[8:length(training3)]
```
After the above data cleansing we were left with 53 variables out of the original 160.

***

## Cross Validation Set

We allocated 30% of the traing data for cross validation to test the accuracy of our model.

```{r tidy=TRUE}
library(caret)
inTrain <- createDataPartition(y = training3$classe, p = 0.7, list = FALSE)
training <- training3[inTrain, ]
crossval <- training3[-inTrain, ]
```
As we still have a lot of variables (53) we used a correlation matrix to identify if there are any highly correlated variables in the data.
Highly correlated variables can be removed thereby improving the efficiency of our process and reducing overfitting.

```{r tidy=TRUE}
library(corrplot)
corM <- cor(training[, -length(training)])
corrplot(corM, order = "FPC", method = "circle", type = "lower", tl.cex = 0.8,  tl.col = rgb(0, 0, 0))
```

The correlation plot identifies correlation by the strength of the color.  As can be seen in the plot it looks like we do have quite a 
few correlated variables.

We used a correlation coefficient r > 0.5 to remove highly correlated variables.

```{r tidy=TRUE}
highCorr <- findCorrelation(corM, cutoff = 0.5)
trainingCor <- training[, -highCorr]
ncol(trainingCor)
```
Our training data now has 24 non-correlated variables from the original 53.  If we plot a correlation matrix we confirm the data is 
not highly correlated.

```{r tidy=TRUE}
corM2 <- cor(trainingCor[, -length(trainingCor)])
corrplot(corM2, order = "FPC", method = "circle", type = "lower", tl.cex = 0.8,  tl.col = rgb(0, 0, 0))
```

***

## Model Fitting

We fit a random forest model to predict the 'classe' variable using everything else as a predictor.  We used random forest as that is stated
to be the most accurate in our lecture notes.

```{r tidy=TRUE}
library(caret)
library(kernlab)
library(randomForest)
model <- randomForest(classe ~ ., data = trainingCor)
```
***

## Model Accuracy using the Cross Validation Data

Now we want to test the accuracy of our model so we crossvalidate the model using the remaining 30% of data.

```{r tidy=TRUE}
predictCrossVal <- predict(model, crossval)
confusionMatrix(crossval$classe, predictCrossVal)
```

The accuracy of our model is tested by comparing the predicted values from our model to the actual values in the CV data.
```{r tidy=TRUE}
accuracy <- sum((predictCrossVal == crossval$classe))/dim(crossval)[1]
```
Our model accuracy is 98.59%.

We also want to know the out-of-sample error.  This is the complement of accuracy.
```{r tidy=TRUE}
1-accuracy
```
In our case the out-of-sample error is 1.4%.

***

## Use the Testing Data to Predict Weight Lifting Performance

We then apply the same data cleaning treatment to the final testing data.

```{r tidy=TRUE}
test1 <- read.csv("./data/pml-testing.csv", na.strings= c("NA",""," "))
test2 <- apply(test1, 2, function(x) {sum(is.na(x))})
test3 <- test1[,which(test2 == 0)]
test3 <- test3[8:length(test3)]
```

Now we predict the classes of the test set on the cleaned testing set.
```{r tidy=TRUE}
predictTest <- predict(model, test3)
print(predictTest)
```
Save the predicted results in the 20 files for Coursera grading submission.
```{r echo=FALSE}
setwd("C:/Users/buckleyb/Documents/Personal/Courses/Johns Hopkins Practical Machine Learning/Code Assignment/predictions")
```
```{r tidy=TRUE}
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}
pml_write_files(predictTest)
```

***

## Conclusion

We used a random forest prediction model on cleaned and uncorrelated data from Groupware.  Our model had an accuracy of 98.59% with an 
out-of-sample error of 1.4%.  When applied to the testing data set our submitted prediction results to Coursera were all correctly identified.
